---
title: "Week44 part two"
author: "562770bf"
date: "2024-11-4"
output: html_document
---
This is the second Readme file for the Data Science and HR analytics course, trying out experiments on the code for week 45.

So let's get started. 
```{r}
browser <- read.csv("/Users/annelies2/Downloads/web-browsers.csv")
dim(browser)
head(browser)
```
This code reads a CSV file into R, displays its dimensions (number of rows and columns), and shows the first six rows of the data.
```{r}
mean(browser$spend); var(browser$spend)/1e4; sqrt(var(browser$spend)/1e4)
  B <- 1000
  mub <- c()
  for (b in 1:1000){
    samp_b <- sample.int(nrow(browser), replace=TRUE)
    mub <- c(mub, mean(browser$spend[samp_b]))
  }
  sd(mub)
```
This code performs a bootstrap resampling procedure 1000 times to estimate the mean of the `spend` variable in the `browser` data frame and then calculates the standard deviation of the resampled means.
```{r}
# Experiment 1: Varying the number of bootstrap samples (B)
B_values <- c(500, 1000, 5000)  # Different values for B
for (B in B_values) {
  mub <- c()
  for (b in 1:B) {
    samp_b <- sample.int(nrow(browser), replace=TRUE)
    mub <- c(mub, mean(browser$spend[samp_b]))
  }
  cat("B =", B, "SD of bootstrap means:", sd(mub), "\n")
}
# Experiment 2: Using sample() instead of sample.int() for resampling
B <- 1000
mub <- c()
for (b in 1:B) {
  samp_b <- sample(1:nrow(browser), size=nrow(browser), replace=TRUE)  # Random resampling
  mub <- c(mub, mean(browser$spend[samp_b]))
}
cat("SD of bootstrap means with sample() method:", sd(mub), "\n")
# Experiment 3: Calculate a 95% confidence interval from bootstrap means
B <- 1000
mub <- c()
for (b in 1:B) {
  samp_b <- sample.int(nrow(browser), replace=TRUE)
  mub <- c(mub, mean(browser$spend[samp_b]))
}

# Confidence interval
CI <- quantile(mub, c(0.025, 0.975))  # 95% confidence interval
cat("95% Confidence Interval:", CI, "\n")
# Experiment 4: Compare bootstrap means to actual mean
actual_mean <- mean(browser$spend)  # Actual mean of the 'spend' variable
cat("Actual mean:", actual_mean, "\n")

B <- 1000
mub <- c()
for (b in 1:B) {
  samp_b <- sample.int(nrow(browser), replace=TRUE)
  mub <- c(mub, mean(browser$spend[samp_b]))
}

# Compare bootstrap mean to actual mean
bootstrap_mean <- mean(mub)
cat("Bootstrap mean:", bootstrap_mean, "\n")
```
From the experiments, I learned that:

Increasing the number of bootstrap samples tends to reduce the variability in the resampled means, which results in a more stable and reliable estimate of the population mean. As `B` increases, the standard deviation of the bootstrap means generally decreases.
   
Using `sample()` instead of `sample.int()` for resampling produced nearly identical results in terms of the standard deviation of the bootstrap means, indicating that both methods can be used interchangeably for resampling in this case.

Bootstrap confidence intervals provide a range of plausible values for the population mean, offering valuable insights into the uncertainty around the estimate. A 95% confidence interval based on bootstrap resampling captures the variability in the data and can help assess the precision of the mean estimate.

The bootstrap mean was very close to the actual population mean of the `spend` variable, demonstrating that the bootstrap method is effective in approximating population parameters even with limited data.
```{r}
  h <- hist(mub)
  xfit <- seq(min(mub), max(mub), length = 40) 
  yfit <- dnorm(xfit, mean = mean(browser$spend), sd = sqrt(var(browser$spend)/1e4)) 
  yfit <- yfit * diff(h$mids[1:2]) * length(mub) 
  lines(xfit, yfit, col = "black", lwd = 2)
```
First line creates a histogram of the bootstrap sample means (mub) and stores it in the object h. This helps visualize the distribution of the bootstrap means.
Xfit generates a sequence of 40 evenly spaced points (xfit) between the minimum and maximum values of the mub vector. These points will be used to evaluate the fitted normal distribution.
Yfit calculates the probability density values of the normal distribution (dnorm) at each point in xfit, assuming a normal distribution with the mean and adjusted standard deviation based on the original.
Fourth line scales the normal density to match the histogram's area:
diff(h$mids[1:2]): The width of a histogram bin, ensuring the density matches the width of the bars in the histogram.
length(mub): Adjusts the density for the total number of bootstrap samples to scale the normal curve appropriately to match the histogram's total count.browser$spend data.
Fifth line adds the fitted normal curve to the histogram, using the calculated points (xfit, yfit) and styling it as a black line with a width of 2.
```{r}
B <- 1000
  betas <- c()
  for (b in 1:1000){
    samp_b <- sample.int(nrow(browser), replace=TRUE)
    reg_b <- glm(log(spend) ~ broadband + anychildren, data=browser[samp_b,])
    betas <- rbind(betas, coef(reg_b))
  }; head(betas, n=3)
  cov(betas[,"broadband"], betas[,"anychildren"])
```
This code performs a bootstrap resampling procedure to estimate the distribution of the coefficients in a regression model. It resamples the data 1000 times, fits a logistic regression model (glm) on each sample, and stores the estimated coefficients (betas). The final part calculates the covariance between the coefficients for the broadband and anychildren variables.
```{r}
B <- 2000  # Increase the number of bootstrap samples
betas <- c()
for (b in 1:B) {
  samp_b <- sample.int(nrow(browser), replace=TRUE)
  reg_b <- glm(log(spend) ~ broadband + anychildren, data=browser[samp_b,])
  betas <- rbind(betas, coef(reg_b))
}
cov(betas[,"broadband"], betas[,"anychildren"])
```
What I learned from this experiment is that increasing the number of bootstrap samples improves the precision of the coefficient estimates. By calculating the covariance between the coefficients, you can also understand the relationship between the predictors (broadband and anychildren). The higher the number of bootstrap samples, the more reliable the covariance estimate becomes.
