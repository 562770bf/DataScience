---
title: "Week45 Lecture 2"
author: "562770bf"
date: "2024-11-11"
output: html_document
---
This is the second Readme file for the Data Science and HR analytics course, trying out experiments on the code for week 45.

So let's get started. 
```{r}
browser <- read.csv("/Users/annelies2/Downloads/web-browsers.csv")
spendy <- glm(log(spend) ~ . -id, data=browser)
round(summary(spendy)$coef,2)
```
This code fits a generalized linear model (glm) to the browser dataset, predicting the log of the spend variable using all the other variables in the dataset except for id. The summary(spendy)$coef part extracts the coefficients from the fitted model, and round(..., 2) rounds them to two decimal places for easier interpretation.
```{r}
#Experiment 1
browser <- read.csv("/Users/annelies2/Downloads/web-browsers.csv")
colnames(browser)
spendy <- glm(log(spend) ~ broadband + anychildren + race + region, data=browser)
round(summary(spendy)$coef, 2)
```
What I learned from this experiment is that when fitting a regression model, you can control which predictors you include by specifying them in the formula (log(spend) ~ broadband + anychildren + income + age). By including only relevant variables, you can assess how each one affects the dependent variable (spend), which is being modeled here as its log transformation. This is helpful for understanding the relationship between spend and other factors such as broadband, anychildren, income, and age.
```{r}
  pval <- summary(spendy)$coef[-1, "Pr(>|t|)"]
  pvalrank <- rank(pval)
  reject <- ifelse(pval< (0.1/9)*pvalrank, 2, 1) 
  png(file = "/Users/annelies2/Downloads/BHAlgoExample.png", width = 600, height = 350)
  plot(pvalrank, pval, ylab="p-value", xlab="p-value rank", pch=16, col=reject)
  lines(pvalrank, (0.1/9)*pvalrank)
  dev.off()
```
This code implements the Benjamini-Hochberg (BH) procedure for controlling the false discovery rate (FDR) in multiple hypothesis testing. It first extracts the p-values from the regression model (spendy) and ranks them. Then, it calculates whether each p-value should be rejected based on the BH procedure (with a threshold of (0.1/9)*pvalrank). The plot visualizes the p-values and their ranks, with points colored based on whether they are rejected (2) or not (1).
```{r}
#Experiment 2
pval <- summary(spendy)$coef[-1, "Pr(>|t|)"]
pvalrank <- rank(pval)
reject <- ifelse(pval < (0.05/6)*pvalrank, 2, 1)  
png(file = "/Users/annelies2/Downloads/BHAlgoExample.png", width = 600, height = 350)
plot(pvalrank, pval, ylab="p-value", xlab="p-value rank", pch=16, col=reject)
lines(pvalrank, (0.05/6)*pvalrank, col="blue")
dev.off()
```
What I learned from this experiment is that the Benjamini-Hochberg procedure adjusts the significance threshold based on the rank of the p-values, allowing more p-values to be considered significant while controlling for the false discovery rate. The plot helps visualize how the p-values and ranks interact and provides insight into which variables are likely to be significant after applying the correction. By changing the threshold (e.g., from 0.1 to 0.05), you can adjust the stringency of the test.

```{r}
SC <- read.csv("/Users/annelies2/Downloads/semiconductor.csv")
dim(SC)
full <- glm(FAIL ~ ., data=SC, family=binomial)
pvals <- summary(full)$coef[-1,4]
```
This code reads in a CSV file (semiconductor.csv) into a data frame called SC and then displays the dimensions of the dataset using dim(SC), which shows the number of rows and columns. After that, it fits a generalized linear model (glm) to the data with a binomial family, using the variable FAIL as the dependent variable and all other variables (.) as predictors. The model is a logistic regression, as indicated by the binomial family.
Next, the code extracts the p-values for each predictor variable (excluding the intercept) using summary(full)$coef[-1,4]. These p-values indicate the statistical significance of each predictor in explaining the variation in FAIL.
```{r}
hist(pvals, xlab="p-value", main="", col="lightblue")
```
The code creates a histogram of the `pvals` vector, which contains p-values from the logistic regression model. It labels the x-axis as "p-value", sets an empty title for the plot, and uses a light blue color for the bars in the histogram. This visualization helps to understand the distribution of p-values from the model's coefficients.
```{r}
fdr_pvals <- p.adjust(pvals, method = "fdr")
```
The function fdr_cut(pvals) is likely intended to apply the False Discovery Rate (FDR) correction to the p-values in the pvals vector. The FDR correction is commonly used to adjust for multiple comparisons and reduce the likelihood of Type I errors (false positives) when performing many statistical tests.